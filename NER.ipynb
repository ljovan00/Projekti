{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9hVFIRSIzJK",
        "outputId": "7a437d42-fea6-408a-c57d-763dc14008fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymJBjZ7TJByX",
        "outputId": "4977f664-7ba3-4f22-944b-25e7a35649ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data sentences DataFrame:\n",
            "                                            sentence\n",
            "0  Kazna medijskom mogulu obnovila raspravu u Mak...\n",
            "1  Neki tvrde da je presuda Veliji Ramkovskom nap...\n",
            "2  Medijski mogul Velija Ramkovski osuđen je na 1...\n",
            "3  Kaznena presuda i zatvorska kazna medijskom mo...\n",
            "4  Ramkovski , bivši vlasnik televizijske postaje...\n",
            "\n",
            "Test data sentences DataFrame:\n",
            "                                            sentence\n",
            "0  Beograd i Priština postigli dogovor o slobodi ...\n",
            "1  Pregovarački timovi Beograda i Prištine usugla...\n",
            "2  Neki tvrde kako su sporazumi korak prema konač...\n",
            "3  Dok vlasti u Beogradu pokušavaju predstaviti p...\n",
            "4  Nakon završetka razgovora u Bruxellesu , šef i...\n",
            "\n",
            "Dev data sentences DataFrame:\n",
            "                                            sentence\n",
            "0       Proces privatizacije na Kosovu pod povećalom\n",
            "1  Kosovo ozbiljno analizira proces privatizacije...\n",
            "2  Feronikel je privatiziran prije pet godina i j...\n",
            "3      Barem na papiru , izgleda kao odlična ideja .\n",
            "4  Vlada prodaje poduzeća , rješava se opterećenj...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import conllu\n",
        "\n",
        "def extract_sentences(file_path):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('# sent_id'):\n",
        "                if current_sentence:\n",
        "                    sentences.append(' '.join(current_sentence))\n",
        "                    current_sentence = []\n",
        "            elif line.startswith('#') or line == '\\n':\n",
        "                continue\n",
        "            else:\n",
        "                token = conllu.parse(line)[0][0]['form']\n",
        "                current_sentence.append(token)\n",
        "\n",
        "    if current_sentence:\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "\n",
        "    return sentences\n",
        "\n",
        "train_sentences = extract_sentences('hr500k-train.conllu')\n",
        "test_sentences = extract_sentences('hr500k-test.conllu')\n",
        "dev_sentences = extract_sentences('hr500k-dev.conllu')\n",
        "\n",
        "train_df = pd.DataFrame({'sentence': train_sentences})\n",
        "test_df = pd.DataFrame({'sentence': test_sentences})\n",
        "dev_df = pd.DataFrame({'sentence': dev_sentences})\n",
        "\n",
        "train_df.to_csv('train_sentences.csv', index=False)\n",
        "test_df.to_csv('test_sentences.csv', index=False)\n",
        "dev_df.to_csv('dev_sentences.csv', index=False)\n",
        "\n",
        "print('Train data sentences DataFrame:')\n",
        "print(train_df.head())\n",
        "\n",
        "print('\\nTest data sentences DataFrame:')\n",
        "print(test_df.head())\n",
        "\n",
        "print('\\nDev data sentences DataFrame:')\n",
        "print(dev_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3hBsbkLpKS3M"
      },
      "outputs": [],
      "source": [
        "import conllu\n",
        "\n",
        "def extract_ner_tags(file_path):\n",
        "    sentences = []\n",
        "    ner_tags = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        current_sentence = []\n",
        "        current_ner_tags = []\n",
        "\n",
        "        for tokenlist in conllu.parse_incr(file):\n",
        "            for token in tokenlist:\n",
        "                if 'misc' in token and token['misc'] is not None and 'NamedEntity' in token['misc'] and token['misc']['NamedEntity'] == 'Yes':\n",
        "                    current_sentence.append(token['form'])\n",
        "                    current_ner_tags.append(token['misc']['NER'])\n",
        "            # End of the sentence\n",
        "            if current_sentence:\n",
        "                sentences.append(\" \".join(current_sentence))\n",
        "                ner_tags.append(current_ner_tags)\n",
        "                current_sentence = []\n",
        "                current_ner_tags = []\n",
        "\n",
        "    return sentences, ner_tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mP8bT4HBKFtz"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_ner_tags = extract_ner_tags('hr500k-train.conllu')\n",
        "\n",
        "test_sentences, test_ner_tags = extract_ner_tags('hr500k-test.conllu')\n",
        "dev_sentences, dev_ner_tags = extract_ner_tags('hr500k-dev.conllu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jzy1mgMUQY6R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "--lsdwtGTAFf"
      },
      "outputs": [],
      "source": [
        "# Combining sentences and NER tags for training data\n",
        "train_data = list(zip(train_sentences, train_ner_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c2MMWfRXUWO4"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad the sentences\n",
        "max_words = 10000  \n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0IPW1BYPUY3B"
      },
      "outputs": [],
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zP73oWexUa11"
      },
      "outputs": [],
      "source": [
        "maxlen = max(len(seq) for seq in train_sequences)\n",
        "train_data = pad_sequences(train_sequences, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
        "dev_data = pad_sequences(dev_sequences, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NEJUr_bgUaz-"
      },
      "outputs": [],
      "source": [
        "# Converting NER tags to one-hot encoding\n",
        "ner_tags = set(tag for tags in train_ner_tags for tag in tags)\n",
        "tag_to_index = {tag: i for i, tag in enumerate(ner_tags)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AreIWYNHUaxa"
      },
      "outputs": [],
      "source": [
        "train_labels = [[tag_to_index[tag] for tag in tags] for tags in train_ner_tags]\n",
        "train_labels = pad_sequences(train_labels, maxlen=maxlen, padding='post')\n",
        "\n",
        "test_labels = [[tag_to_index[tag] for tag in tags] for tags in test_ner_tags]\n",
        "test_labels = pad_sequences(test_labels, maxlen=maxlen, padding='post')\n",
        "\n",
        "dev_labels = [[tag_to_index[tag] for tag in tags] for tags in dev_ner_tags]\n",
        "dev_labels = pad_sequences(dev_labels, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-AchZZdUUauI"
      },
      "outputs": [],
      "source": [
        "# Bidirectional LSTM model\n",
        "embedding_dim = 100 \n",
        "output_dim = len(ner_tags)  # Number of NER tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SPeXbqzAUarP"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "model.add(Dense(output_dim, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5jTqVCZtUlFg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMIyGIaVUk_9",
        "outputId": "5ef50c79-b377-4b00-bbb6-7303cbe03cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "204/204 [==============================] - 20s 73ms/step - loss: 0.4053 - accuracy: 0.9082 - val_loss: 0.2645 - val_accuracy: 0.9205\n",
            "Epoch 2/5\n",
            "204/204 [==============================] - 4s 20ms/step - loss: 0.2496 - accuracy: 0.9193 - val_loss: 0.2237 - val_accuracy: 0.9238\n",
            "Epoch 3/5\n",
            "204/204 [==============================] - 3s 14ms/step - loss: 0.2298 - accuracy: 0.9223 - val_loss: 0.2108 - val_accuracy: 0.9259\n",
            "Epoch 4/5\n",
            "204/204 [==============================] - 3s 12ms/step - loss: 0.2211 - accuracy: 0.9248 - val_loss: 0.2097 - val_accuracy: 0.9276\n",
            "Epoch 5/5\n",
            "204/204 [==============================] - 3s 12ms/step - loss: 0.2154 - accuracy: 0.9261 - val_loss: 0.2080 - val_accuracy: 0.9275\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f9b38167ee0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=5, validation_data=(dev_data, dev_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BbcNRSt2IbnG"
      },
      "outputs": [],
      "source": [
        "# Create reverse mapping from index to tag\n",
        "index_to_tag = {i: tag for tag, i in tag_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nquaZi6EIEKZ",
        "outputId": "25b0ee32-f7d6-4178-8873-3b63d5cd4d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " B-deriv-per       0.00      0.00      0.00        38\n",
            "       B-loc       0.96      0.99      0.98     36208\n",
            "      B-misc       0.00      0.00      0.00       350\n",
            "       B-org       0.27      0.21      0.24       740\n",
            "       B-per       0.31      0.22      0.25       692\n",
            "       I-loc       0.00      0.00      0.00       165\n",
            "      I-misc       0.00      0.00      0.00       509\n",
            "       I-org       0.26      0.47      0.34       556\n",
            "       I-per       0.08      0.00      0.00       441\n",
            "\n",
            "    accuracy                           0.92     39699\n",
            "   macro avg       0.21      0.21      0.20     39699\n",
            "weighted avg       0.89      0.92      0.90     39699\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np\n",
        "# Predict on test data\n",
        "test_predictions = model.predict(test_data)\n",
        "\n",
        "# Convert predictions to labels\n",
        "test_pred_labels = [[index_to_tag[i] for i in np.argmax(pred, axis=-1)] for pred in test_predictions]\n",
        "test_true_labels = [[index_to_tag[i] for i in seq] for seq in test_labels]\n",
        "\n",
        "# Flatten the lists for classification_report\n",
        "flat_test_true_labels = [tag for sublist in test_true_labels for tag in sublist]\n",
        "flat_test_pred_labels = [tag for sublist in test_pred_labels for tag in sublist]\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(flat_test_true_labels, flat_test_pred_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
